{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "\n",
    "Apply a linear classifier, `LogisticRegression()` to the heart disease dataset.\n",
    "\n",
    "The data is hosted on UCI https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "\n",
    "We will need `processed-hungarian.data` from the UCI website (also included on D2L)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #ignoring some deprication warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('processed.hungarian.data', \n",
    "                   na_values='?', \n",
    "                   names=[ 'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',\n",
    "                            'restecg', 'thalach', 'exang', 'oldpeak', 'slope',\n",
    "                            'ca', 'thal', 'num'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set interpretation\n",
    "\n",
    "The first 13 columns are features to predict heart disease, column 14 (num) is the target vector.\n",
    "\n",
    "num: diagnosis of heart disease (angiographic disease status)  \n",
    "- Value 0 (no disease): < 50% diameter narrowing\n",
    "- Value 1 (heart disease) : > 50% diameter narrowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop columns with many missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['slope', 'ca', 'thal'])\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing values with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(data.mean())\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A pairplot to get an overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, hue='num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(by='num').sex.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the data loading code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_heart_disease():\n",
    "    '''Load and pre-process heart disease data\n",
    "    \n",
    "    assumes processed.hungarian.data file is present.\n",
    "    \n",
    "    download from\n",
    "    https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\n",
    "    \n",
    "    return: data(DataFrame)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    data = pd.read_csv('processed.hungarian.data', \n",
    "                   na_values='?', \n",
    "                   names=[ 'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',\n",
    "                            'restecg', 'thalach', 'exang', 'oldpeak', 'slope',\n",
    "                            'ca', 'thal', 'num'])\n",
    "    \n",
    "    # drop columns with many missing data\n",
    "    data = data.drop(columns=['slope', 'ca', 'thal'])\n",
    "    \n",
    "    # fill in remaining missing data with mean() per column\n",
    "    data = data.fillna(data.mean())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = load_heart_disease()\n",
    "\n",
    "data.equals(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature matrix and target vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns='num')\n",
    "y = data['num']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.1,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=31)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: what does stratify=y do? This parameter makes sure that the data is split with the same proportion of classes as the original dataset. For example, if 20% of the data is class 0 and 80% is class 1, then the training set and testing set will also have 20% class 0 and 80% class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the division of classes is the same for both the training and testing data (63-64% for class 1).\n",
    "\n",
    "We now set aside the testing data and use the training data to validate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply `LogisticRegression()` and evaluate the performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation \n",
    "\n",
    "Are we over- or underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `cross_val_score` only gives us the validation results. To check if we are over- or underfitting, we also need the training results. For this, we can use `cross_validate` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(model, X_train, y_train, cv=5, \n",
    "                        scoring='accuracy',\n",
    "                       return_train_score=True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_pair in [ ('train_score', 'train_score'), ('test_score', 'validation_score')]:\n",
    "    print('{}= {:.3f}'.format(label_pair[1], scores[label_pair[0]].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty close. We are likely underfitting and would need a more complex model to increase validation set accuracy.\n",
    "\n",
    "Lets try and play with the hyperparameter, `C`. Note that the default value for `C` is 1. To increase the model complexity, we need to increase the value of `C`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=100, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "scores = cross_validate(model, X_train, y_train, cv=5, \n",
    "                        scoring='accuracy',\n",
    "                       return_train_score=True)\n",
    "for label_pair in [ ('train_score', 'train_score'), ('test_score', 'validation_score')]:\n",
    "    print('{}= {:.3f}'.format(label_pair[1], scores[label_pair[0]].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not seem to help. We need a more complex model. We will get back to it later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The confusion matrix on the test set \n",
    "\n",
    "Your thoughts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(mat, xticklabels=['no heart disease', 'heart disease'],  yticklabels=['no heart disease', 'heart disease'], square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two cases were the classifier says *no heart disease* but these patients do have a *heart disease*. These are  false negatives - we miss patients with the disease.\n",
    "\n",
    "There are three cases were the classifier says *heart disease* but these patients do *not* have a *heart disease*. These are false positives - we diagnose disease when there is none."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the different scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_validate(model, X_train, y_train, cv=5, \n",
    "                        scoring='accuracy',\n",
    "                       return_train_score=True)\n",
    "\n",
    "print('training accuracy (all data) {:.3f}'.format(model.score(X_train, y_train)))\n",
    "\n",
    "print('training accuracy (cross-validation) {:.3f}'.format(scores['train_score'].mean()))\n",
    "\n",
    "print('validation accuracy (cross-validation) {:.3f}'.format(scores['test_score'].mean()))\n",
    "\n",
    "print('test accuracy (new data) {:.3f}'.format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
